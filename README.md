# iFood Web Scraper

Um scraper profissional para extrair dados de categorias e restaurantes da plataforma iFood, desenvolvido com Playwright para simular comportamento humano e evitar detecÃ§Ã£o.

## CaracterÃ­sticas Principais

- **Arquitetura Modular**: OrganizaÃ§Ã£o profissional com separaÃ§Ã£o de responsabilidades
- **Comportamento Humano**: Simula aÃ§Ãµes humanas com delays aleatÃ³rios e movimentos de mouse
- **Controle de Duplicatas**: Sistema de hash MD5 para evitar dados duplicados
- **OrganizaÃ§Ã£o por Categoria**: Arquivos CSV separados para cada tipo de comida
- **Sistema de Logs Completo**: Rastreamento detalhado de todas as operaÃ§Ãµes
- **Tratamento de Erros Robusto**: Retry automÃ¡tico e screenshots para debug

## Estrutura do Projeto

```
ifood/
â”œâ”€â”€ main.py                    # Ponto de entrada principal
â”œâ”€â”€ INSTALL.md                 # Guia de instalaÃ§Ã£o detalhado
â”œâ”€â”€ .env.example              # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore                # ConfiguraÃ§Ãµes Git
â”œâ”€â”€ src/                      # CÃ³digo fonte
â”‚   â”œâ”€â”€ config/               # ConfiguraÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ settings.py       # ConfiguraÃ§Ãµes gerais
â”‚   â”‚   â””â”€â”€ browser_config.py # ConfiguraÃ§Ãµes do navegador
â”‚   â”œâ”€â”€ models/               # Modelos de dados
â”‚   â”‚   â”œâ”€â”€ category.py       # Modelo de categoria
â”‚   â”‚   â””â”€â”€ restaurant.py     # Modelo de restaurante
â”‚   â”œâ”€â”€ scrapers/             # Scrapers especializados
â”‚   â”‚   â”œâ”€â”€ base.py          # Classe base
â”‚   â”‚   â”œâ”€â”€ ifood_scraper.py # Scraper principal
â”‚   â”‚   â”œâ”€â”€ category_scraper.py # ExtraÃ§Ã£o de categorias
â”‚   â”‚   â””â”€â”€ restaurant_scraper.py # ExtraÃ§Ã£o de restaurantes
â”‚   â””â”€â”€ utils/                # UtilitÃ¡rios
â”‚       â”œâ”€â”€ database.py       # Gerenciamento de dados
â”‚       â”œâ”€â”€ error_handler.py  # Tratamento de erros
â”‚       â”œâ”€â”€ human_behavior.py # SimulaÃ§Ã£o humana
â”‚       â”œâ”€â”€ helpers.py        # FunÃ§Ãµes auxiliares
â”‚       â””â”€â”€ logger.py         # Sistema de logs
â”œâ”€â”€ config/                   # Arquivos de configuraÃ§Ã£o
â”‚   â””â”€â”€ requirements.txt      # DependÃªncias Python
â”œâ”€â”€ data/                     # Dados extraÃ­dos (CSV e JSON)
â”œâ”€â”€ logs/                     # Arquivos de log atuais
â”œâ”€â”€ docs/                     # DocumentaÃ§Ã£o adicional
â”‚   â”œâ”€â”€ CORRECOES_APLICADAS.md
â”‚   â”œâ”€â”€ MELHORIAS_DADOS.md
â”‚   â””â”€â”€ MELHORIAS_POPUP.md
â”œâ”€â”€ examples/                 # Exemplos de uso
â”‚   â””â”€â”€ basic_usage.py        # Exemplo bÃ¡sico completo
â”œâ”€â”€ tools/                    # Ferramentas auxiliares
â”‚   â”œâ”€â”€ check_dependencies.py # Verificador de dependÃªncias
â”‚   â””â”€â”€ data_analyzer.py      # Analisador de dados
â”œâ”€â”€ archive/                  # Arquivos antigos
â”‚   â”œâ”€â”€ old_logs/            # Logs histÃ³ricos
â”‚   â””â”€â”€ screenshots/         # Screenshots de erro
â””â”€â”€ temp_data/               # Dados temporÃ¡rios
```

## InstalaÃ§Ã£o

Para instalaÃ§Ã£o completa e detalhada, consulte o [Guia de InstalaÃ§Ã£o](INSTALL.md).

### InstalaÃ§Ã£o RÃ¡pida

```bash
# 1. Instalar dependÃªncias
pip install -r config/requirements.txt

# 2. Configurar Playwright
playwright install
sudo playwright install-deps  # Linux/WSL

# 3. Verificar instalaÃ§Ã£o
python tools/check_dependencies.py
```

## Uso

### 1. Extrair Categorias

Coleta todas as categorias de comida disponÃ­veis na cidade:

```bash
python main.py --mode categories --city "Nome da Cidade"
```

**Exemplo:**
```bash
python main.py --mode categories --city "SÃ£o Paulo"
```

### 2. Extrair Restaurantes ğŸ¯

Coleta dados dos restaurantes de uma categoria especÃ­fica:

```bash
python main.py --mode restaurants --city "Nome da Cidade"
```

O sistema apresentarÃ¡ um menu interativo para selecionar a categoria desejada.

## Dados ExtraÃ­dos

### Categorias
- **Nome**: Nome da categoria (ex: "Japonesa", "Italiana")
- **URL**: Link direto para a categoria
- **Slug**: Identificador Ãºnico
- **Cidade**: Cidade de origem
- **Ãcone**: URL do Ã­cone da categoria

### Restaurantes ğŸš€
- **Nome**: Nome do restaurante
- **Categoria**: Tipo de comida
- **AvaliaÃ§Ã£o**: Rating de 0 a 5 estrelas
- **Tempo de Entrega**: Tempo estimado (ex: "30-40 min")
- **Taxa de Entrega**: Custo de entrega (ex: "GrÃ¡tis", "R$ 4,99")
- **DistÃ¢ncia**: DistÃ¢ncia do restaurante (ex: "1.2 km")
- **URL**: Link direto para o cardÃ¡pio do restaurante
- **EndereÃ§o**: LocalizaÃ§Ã£o (quando disponÃ­vel)

### Produtos/CardÃ¡pio ğŸ½ï¸
- **Nome**: Nome do produto/prato
- **DescriÃ§Ã£o**: Detalhes e ingredientes
- **PreÃ§o**: Valor atual de venda
- **PreÃ§o Original**: Valor antes de promoÃ§Ãµes
- **Categoria**: Tipo de produto (entrada, prato principal, etc)
- **Disponibilidade**: Se estÃ¡ disponÃ­vel para pedido
- **Imagem**: URL da foto do produto
- **Tempo de Preparo**: Quando informado
- **Calorias**: InformaÃ§Ã£o nutricional
- **Tags**: PromoÃ§Ã£o, Vegano, Novo, etc

## Arquivos Gerados

### Estrutura de Dados Organizada ğŸ“

```
data/
â”œâ”€â”€ categories/                              # ğŸ“‚ Categorias de comida
â”‚   â””â”€â”€ ifood_data_categories.csv           #     Lista completa de categorias
â”œâ”€â”€ restaurants/                             # ğŸª Dados dos restaurantes
â”‚   â”œâ”€â”€ ifood_data_restaurantes_japonesa.csv #     Restaurantes japoneses
â”‚   â”œâ”€â”€ ifood_data_restaurantes_italiana.csv #     Restaurantes italianos
â”‚   â””â”€â”€ ifood_data_restaurantes_brasileira.csv #   Restaurantes brasileiros
â”œâ”€â”€ products/                                # ğŸ½ï¸ CardÃ¡pios e produtos
â”‚   â”œâ”€â”€ ifood_data_produtos_burger_king.csv #     CardÃ¡pio do Burger King
â”‚   â”œâ”€â”€ ifood_data_produtos_pizza_hut.csv   #     CardÃ¡pio da Pizza Hut
â”‚   â””â”€â”€ ifood_data_produtos_sushi_house.csv #     CardÃ¡pio do Sushi House
â””â”€â”€ ifood_data_metadata.json                # ğŸ“Š EstatÃ­sticas gerais
```

### Exemplo de CSV - Restaurantes

```csv
id,nome,categoria,avaliacao,tempo_entrega,taxa_entrega,distancia,url,endereco,extracted_at
abc123,Sushi House,Japonesa,4.8,30-40 min,GrÃ¡tis,1.2 km,https://ifood.com.br/delivery/...,Rua das Flores 123,2025-07-01T20:30:00
```

## ConfiguraÃ§Ãµes

### Browser

- **Modo**: Headless por padrÃ£o (pode ser alterado)
- **User-Agent**: Simula navegadores reais
- **Viewport**: 1920x1080 para melhor compatibilidade
- **Timeout**: 30 segundos para carregamento de pÃ¡ginas

### Comportamento Humano

- **Delays AleatÃ³rios**: 1-3 segundos entre aÃ§Ãµes
- **Movimentos de Mouse**: SimulaÃ§Ã£o de interaÃ§Ã£o natural
- **Scroll Inteligente**: Carregamento progressivo de conteÃºdo
- **Retry AutomÃ¡tico**: 3 tentativas em caso de falha

## Sistema de Logs

### Nova Arquitetura de Logs ğŸ¯

O sistema foi otimizado para reduzir a quantidade de arquivos de log:

- **Um arquivo por dia**: Todos os componentes compartilham o mesmo arquivo
- **RotaÃ§Ã£o automÃ¡tica**: Logs antigos sÃ£o removidos apÃ³s 7 dias
- **ConsolidaÃ§Ã£o inteligente**: MÃºltiplas execuÃ§Ãµes no mesmo dia sÃ£o agrupadas

### Estrutura de Logs

```
logs/
â””â”€â”€ ifood_scraper_20250701.log    # Todos os logs do dia em um Ãºnico arquivo

archive/
â”œâ”€â”€ old_logs/                      # Logs histÃ³ricos (limpeza automÃ¡tica)
â””â”€â”€ screenshots/                   # Screenshots de erro para debug
```

### ConfiguraÃ§Ãµes

No arquivo `src/config/settings.py`:
- `log_retention_days`: Dias para manter logs (padrÃ£o: 7)
- `log_level`: NÃ­vel de log (INFO, DEBUG, WARNING, ERROR)
- `log_to_console`: Mostrar logs no console
- `log_to_file`: Salvar logs em arquivo

### Ferramentas de ManutenÃ§Ã£o

```bash
# Limpar logs antigos manualmente
python tools/cleanup_logs.py
```

OpÃ§Ãµes disponÃ­veis:
1. Limpar logs com mais de 7 dias
2. Remover TODOS os logs arquivados
3. Consolidar logs do mesmo dia
4. Limpeza completa (1 + 3)

## Controle de Qualidade

### PrevenÃ§Ã£o de Duplicatas

- **Hash MD5**: Gerado a partir de nome + categoria + cidade
- **VerificaÃ§Ã£o AutomÃ¡tica**: Antes de cada inserÃ§Ã£o
- **Cache em MemÃ³ria**: Para verificaÃ§Ã£o rÃ¡pida

### ValidaÃ§Ã£o de Dados

- **Campos ObrigatÃ³rios**: Nome e categoria sempre preenchidos
- **Parsing Inteligente**: ExtraÃ§Ã£o de dados estruturados
- **Fallback Seguro**: Valores padrÃ£o quando dados nÃ£o disponÃ­veis

## Troubleshooting

### Problemas Comuns

**1. Erro de DependÃªncias do Sistema**
```bash
# Instalar dependÃªncias necessÃ¡rias
sudo apt-get install libnss3 libnspr4 libasound2
```

**2. Timeout de NavegaÃ§Ã£o**
- Verifique a conexÃ£o de internet
- Aumente o timeout nas configuraÃ§Ãµes
- Execute em horÃ¡rios de menor trÃ¡fego

**3. Elementos NÃ£o Encontrados**
- O sistema tenta mÃºltiplos seletores automaticamente
- Screenshots sÃ£o salvos para anÃ¡lise
- Logs detalhados mostram a causa

### Debug AvanÃ§ado

- **Screenshots**: Salvos em `archive/` em caso de erro
- **HTML Source**: Capturado para anÃ¡lise posterior
- **Network Logs**: Monitoramento de requisiÃ§Ãµes
- **Verificador**: `python tools/check_dependencies.py`
- **Analisador**: `python tools/data_analyzer.py`

## LimitaÃ§Ãµes e ConsideraÃ§Ãµes

### Performance

- **Velocidade**: Limitada pelos delays humanos (necessÃ¡rios para evitar detecÃ§Ã£o)
- **MemÃ³ria**: Otimizada para processar grandes volumes
- **ConcorrÃªncia**: Preparado para paralelizaÃ§Ã£o futura

### Ã‰tica e Legalidade

- **Respeito ao robots.txt**: Verificar polÃ­ticas do site
- **Rate Limiting**: Delays implementados para nÃ£o sobrecarregar servidores
- **Uso ResponsÃ¡vel**: Apenas para fins educacionais e de pesquisa

## EstatÃ­sticas de Exemplo

Baseado no arquivo de metadados atual:

```json
{
  "statistics": {
    "categories": {
      "total_saved": 15,
      "total_duplicates": 90,
      "last_update": "2025-07-01T20:01:04"
    },
    "restaurants": {
      "total_saved": 156,
      "total_duplicates": 0,
      "last_update": "2025-07-01T20:50:36"
    }
  }
}
```

## Desenvolvimento Futuro

### Ferramentas DisponÃ­veis

- **Verificador de DependÃªncias**: `tools/check_dependencies.py`
- **Analisador de Dados**: `tools/data_analyzer.py`
- **Exemplos de Uso**: `examples/basic_usage.py`

### Melhorias Planejadas

- **ParalelizaÃ§Ã£o**: MÃºltiplas categorias simultaneamente
- **API REST**: Interface web para controle
- **Dashboard**: VisualizaÃ§Ã£o dos dados coletados
- **Agendamento**: ExecuÃ§Ã£o automÃ¡tica programada

### ContribuiÃ§Ãµes

Para contribuir com o projeto:

1. Fork o repositÃ³rio
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Abra um Pull Request

## LicenÃ§a

Este projeto Ã© desenvolvido para fins educacionais. Use com responsabilidade e respeite os termos de uso dos sites.

## Suporte

Para dÃºvidas ou problemas:

- **InstalaÃ§Ã£o**: Consulte `INSTALL.md`
- **VerificaÃ§Ã£o**: Execute `python tools/check_dependencies.py`
- **AnÃ¡lise**: Use `python tools/data_analyzer.py`
- **Logs**: Verifique `logs/` e `archive/old_logs/`
- **DocumentaÃ§Ã£o**: Consulte `docs/` para detalhes tÃ©cnicos

---

**Desenvolvido com foco em qualidade, performance e Ã©tica no web scraping.**